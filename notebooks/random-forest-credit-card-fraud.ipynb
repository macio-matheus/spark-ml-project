{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Big Data\n",
    "\n",
    "Julio Sales <br/>\n",
    "Mácio Matheus<br/>\n",
    "Victor Outtes\n",
    "\n",
    "Dataset: https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import RandomForest as  cls_rf\n",
    "from time import *\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando o contexto do spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()\n",
    "sc = pyspark.SparkContext(appName=\"projeto\")\n",
    "session = pyspark.sql.SparkSession.builder.config(conf=SparkConf())\n",
    "spark = session.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando o dataset do hdfs do hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: decimal(10,0) (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n",
      "Total number of rows: 284807\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.options(header = \"true\", inferschema = \"true\").csv('hdfs://cluster-001-m/user/victor_outtes/creditcard.csv')\n",
    "df.printSchema()\n",
    "print(\"Total number of rows:\", df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset 70-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de linhas do dataset de treinamento: 199492\n",
      "Quantidade de linhas do dataset de testes: 85315\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_RATIO = 0.7\n",
    "\n",
    "# A ultima coluna contém o target\n",
    "transformed_df = df.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[0:-1])))\n",
    "\n",
    "# Dividindo o dataset\n",
    "splits = [TRAIN_DATA_RATIO, 1.0 - TRAIN_DATA_RATIO]\n",
    "train_data, test_data = transformed_df.randomSplit(splits, RANDOM_SEED)\n",
    "print(\"Quantidade de linhas do dataset de treinamento: %d\" % train_data.count())\n",
    "print(\"Quantidade de linhas do dataset de testes: %d\" % test_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treino e parametrização do modelo random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo do treinamento: 23.769 s\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 13579\n",
    "NUM_TREES = 3\n",
    "MAX_DEPTH = 4\n",
    "MAX_BINS = 32\n",
    "t1 = time()\n",
    "# Treinando a random forest\n",
    "model = cls_rf.trainClassifier(train_data, numClasses=2, categoricalFeaturesInfo={}, numTrees=NUM_TREES, maxDepth=MAX_DEPTH, maxBins=MAX_BINS, seed=RANDOM_SEED)\n",
    "\n",
    "t2 = time()\n",
    "t_end = t2 - t1\n",
    "print(\"Tempo do treinamento: %.3f s\" % t_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predição e medição da acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 99.945%\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_data.map(lambda x: x.features))\n",
    "labels_and_predictions = test_data.map(lambda x: x.label).zip(predictions)\n",
    "accuracy = labels_and_predictions.filter(lambda x: x[0] == x[1]).count() / float(test_data.count())\n",
    "print(\"Model accuracy: %.3f%%\" % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas: precision recall / curva roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot import name 'hashtable'\n",
      "Area PR curve: 83\n",
      "Area ROC curve: 95.021\n"
     ]
    }
   ],
   "source": [
    "metrics = BinaryClassificationMetrics(labels_and_predictions)\n",
    "print(\"Area PR curve: %.f\" % (metrics.areaUnderPR * 100))\n",
    "print(\"Area ROC curve: %.3f\" % (metrics.areaUnderROC * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
